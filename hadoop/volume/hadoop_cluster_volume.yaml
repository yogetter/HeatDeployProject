heat_template_version: ocata

parameters:
  key_name:
    type: string
  flavor:
    type: string
  image:
    type: string
  private_net:
    type: string
  public_net:
    type: string
    default: ''
  slave_count:
    type: number
    label: instances count
    description: Number of instances
  volume_size:
    type: number
    label: volume size
    description: size of volume(gb)

conditions:
  has_public_network:
    not:
      equals:
      - {get_param: public_net}
      - ''

resources:
################### install java && pre-config ####################
  security_group:
    type: OS::Neutron::SecurityGroup
    properties:
      description: Add security group rules for hadoop cluster
      rules:
        - remote_ip_prefix: 0.0.0.0/0
          protocol: tcp
          port_range_min: 22
          port_range_max: 22
        - remote_ip_prefix: 0.0.0.0/0
          protocol: icmp
        - remote_ip_prefix: 0.0.0.0/0
          protocol: tcp
          port_range_min: 50070
          port_range_max: 50070
        - remote_ip_prefix: 0.0.0.0/0
          protocol: tcp
          port_range_min: 9000
          port_range_max: 9000
        - remote_ip_prefix: 0.0.0.0/0
          protocol: tcp
          port_range_min: 50075
          port_range_max: 50075

  pre_install_master:
    type: OS::Heat::SoftwareConfig
    properties:
      inputs:
        - name: master_ip
        - name: slave_ip
      group: script
      config: |
        #!/bin/bash
        echo "" > /etc/hosts
        echo "127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4" >> /etc/hosts
        echo "::1         localhost localhost.localdomain localhost6 localhost6.localdomain6" >> /etc/hosts
        echo ${master_ip} hadoop-master >> /etc/hosts
        echo ${slave_ip} > /root/slave_ip
        rm /root/.ssh/known_hosts
        index=0
        for i in $(cat /root/slave_ip | grep -o "[0-9]\{1,3\}.[0-9]\{1,3\}.[0-9]\{1,3\}.[0-9]\{1,3\}")
        do
             index=$((index+1))
             echo "$i hadoop-slave$index" >> /etc/hosts
        done
        #dnf install -y java-1.8.0-openjdk.x86_64 java-1.8.0-openjdk-devel.x86_64 wget 
        sed -i '1i export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.181.b15-0.fc27.x86_64/jre/' /root/.bashrc
        ssh -o StrictHostKeyChecking=no hadoop-master -t  "exit"
  pre_install_slave:
    type: OS::Heat::SoftwareConfig
    properties:
      group: script
      config: |
        #!/bin/bash
        #dnf install -y java-1.8.0-openjdk.x86_64 java-1.8.0-openjdk-devel.x86_64 wget 
        sed -i '1i export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.181.b15-0.fc27.x86_64/jre/' /root/.bashrc
################### install java && pre-config ####################

################### Hadoop cluster set ####################
  set_hadoop:
    type: OS::Heat::SoftwareConfig
    properties:
      group: script
      config: |
        #!/bin/bash
        tar -xf /opt/hadoop-2.7.2.tar.gz -C /opt/
        echo -e "<?xml version=\"1.0\" encoding=\"UTF-8\"?> \n\
        <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> \n\
        <configuration> \n\
           <property> \n\
               <name>fs.defaultFS</name> \n\
               <value>hdfs://hadoop-master:9000</value> \n\
           </property> \n\
            <property> \n\
               <name>hadoop.tmp.dir</name> \n\
               <value>/hadoop</value> \n\
           </property> \n\
        </configuration>" > /opt/hadoop-2.7.2/etc/hadoop/core-site.xml
        echo -e "<?xml version=\"1.0\" encoding=\"UTF-8\"?> \n\
        <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> \n\
        <configuration> \n\
           <property> \n\
               <name>dfs.namenode.secondary.http-address</name> \n\
               <value>hadoop-master:50090</value> \n\
           </property> \n\
           <property> \n\
               <name>dfs.namenode.secondary.https-address</name> \n\
               <value>hadoop-master:50091</value> \n\
           </property> \n\
        </configuration>" > /opt/hadoop-2.7.2/etc/hadoop/hdfs-site.xml
        echo -e "<?xml version=\"1.0\" encoding=\"UTF-8\"?> \n\
        <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> \n\
        <configuration> \n\
           <property> \n\
               <name>yarn.nodemanager.aux-services</name> \n\
               <value>mapreduce_shuffle</value> \n\
           </property> \n\
           <property> \n\
               <name>yarn.resourcemanager.hostname</name> \n\
               <value>hadoop-master</value> \n\
           </property> \n\
        </configuration>" > /opt/hadoop-2.7.2/etc/hadoop/yarn-site.xml
        echo -e "<?xml version=\"1.0\" encoding=\"UTF-8\"?> \n\
        <configuration> \n\
           <property> \n\
               <name>mapreduce.framework.name</name> \n\
               <value>yarn</value> \n\
           </property> \n\
        </configuration>" > /opt/hadoop-2.7.2/etc/hadoop/mapred-site.xml
        source /root/.bashrc
        /opt/hadoop-2.7.2/bin/hdfs namenode -format >> /root/run.logs

  set_slaves:
    type: OS::Heat::SoftwareConfig
    properties:
      group: script
      config: |
        #!/bin/bash
        index=0
        echo "" > /opt/hadoop-2.7.2/etc/hadoop/slaves
        for i in $(cat /root/slave_ip | grep -o "[0-9]\{1,3\}.[0-9]\{1,3\}.[0-9]\{1,3\}.[0-9]\{1,3\}")
        do
             index=$((index+1))
             echo hadoop-slave$index >> /opt/hadoop-2.7.2/etc/hadoop/slaves
        done
        index=0
        for i in $(cat /root/slave_ip | grep -o "[0-9]\{1,3\}.[0-9]\{1,3\}.[0-9]\{1,3\}.[0-9]\{1,3\}")
        do
             index=$((index+1))
             scp -r -o StrictHostKeyChecking=no /opt/hadoop-2.7.2 hadoop-slave$index:/opt/
             scp -o StrictHostKeyChecking=no /etc/hosts hadoop-slave$index:/etc/
        done

  run_hadoop:
    type: OS::Heat::SoftwareConfig
    properties:
      group: script
      config: |
        #!/bin/bash
        source /root/.bashrc
        /opt/hadoop-2.7.2/sbin/stop-yarn.sh >> /root/run.logs
        /opt/hadoop-2.7.2/sbin/stop-dfs.sh >> /root/run.logs
        /opt/hadoop-2.7.2/sbin/start-dfs.sh >> /root/run.logs
        /opt/hadoop-2.7.2/sbin/start-yarn.sh  >> /root/run.logs

################### Hadoop cluster set ####################

################### ssh key no pawssord auth ####################
  key_add:
    type: OS::Heat::SoftwareConfig
    properties:
      inputs:
        - name: id_rsa_pub
        - name: user_name
      outputs:
        - name: hostname
      group: script
      config: |
        #!/bin/bash
        echo "${id_rsa_pub}" | su - $user_name -c 'tee -a .ssh/authorized_keys'
        hostname > ${heat_outputs_path}.hostname

  key_gen:
    type: OS::Heat::SoftwareConfig
    properties:
      outputs:
        - name: id_rsa_pub
      group: script
      config: |
        #!/bin/bash
        echo -e "\n" | ssh-keygen -t rsa -N "" 
        cat /root/.ssh/id_rsa.pub > ${heat_outputs_path}.id_rsa_pub
################### ssh key no pawssord auth ####################

################### apply ssh key no pawssord auth ####################
  do_key_gen:
    type: OS::Heat::SoftwareDeployment
    properties:
      config:
        get_resource: key_gen
      server:
        get_resource: master_server

  do_key_add:
    type: OS::Heat::SoftwareDeploymentGroup
    properties:
      input_values:
        user_name: root
        id_rsa_pub:
          get_attr: [do_key_gen, id_rsa_pub]
      config:
        get_resource: key_add
      servers:
        get_attr: [slave_cluster, attributes, slave_id]

  do_key_add_master:
    type: OS::Heat::SoftwareDeployment
    properties:
      input_values:
        user_name: root
        id_rsa_pub:
          get_attr: [do_key_gen, id_rsa_pub]
      config:
        get_resource: key_add
      server:
        get_resource: master_server
################## apply ssh key no pawssord auth ####################

################### apply pre_install ####################
  do_slave_pre_install:
    type: OS::Heat::SoftwareDeploymentGroup
    properties:
      config:
        get_resource: pre_install_slave
      servers:
        get_attr: [slave_cluster, attributes, slave_id]

  do_master_pre_install:
    type: OS::Heat::SoftwareDeployment
    depends_on: do_slave_pre_install
    properties:
      input_values:
        master_ip:
         get_attr: [master_server, first_address]
        slave_ip: 
         get_attr: [slave_cluster, attributes, slave_ip]
      config:
        get_resource: pre_install_master
      server:
        get_resource: master_server
################### apply pre_install ####################

################### run Hadoop ####################
  do_run_set_hadoop:
    type: OS::Heat::SoftwareDeployment
    depends_on: do_master_pre_install
    properties:
      config:
        get_resource: set_hadoop
      server:
        get_resource: master_server

  do_run_set_slaves:
    type: OS::Heat::SoftwareDeployment
    depends_on: do_run_set_hadoop
    properties:
      input_values:
        slave_ip: 
           get_attr: [slave_cluster, attributes, slave_ip]
      config:
        get_resource: set_slaves
      server:
        get_resource: master_server

  do_run_hadoop:
    type: OS::Heat::SoftwareDeployment
    depends_on: do_run_set_slaves
    properties:
      input_values:
        slave_ip: 
           get_attr: [slave_cluster, attributes, slave_ip]
      config:
        get_resource: run_hadoop
      server:
        get_resource: master_server
################### run Hadoop ####################
  slave_cluster:
    type: OS::Heat::ResourceGroup
    properties:
      count: { get_param: slave_count }
      resource_def:
        type: slave_volume.yaml

  master_server:
    type: OS::Nova::Server
    properties:
      name: hadoop-master
      image:
        get_param: image
      flavor:
        get_param: flavor
      key_name:
        get_param: key_name
      networks:
      - port: {get_resource: server_port}
      user_data_format: SOFTWARE_CONFIG
      block_device_mapping: [{"volume_id": { get_resource: volume_storage },
                              "delete_on_termination": true,
                              "device_name": "vda"}]

  server_port:
    type: OS::Neutron::Port
    depends_on: security_group
    properties:
      network_id: { get_param: private_net }
      security_groups: [{ get_resource: security_group }]

  server_floating_ip:
    type: OS::Neutron::FloatingIP
    condition: has_public_network
    properties:
      floating_network_id: { get_param: public_net }
      port_id: { get_resource: server_port }

  volume_storage:
   type: OS::Cinder::Volume
   properties:
    size: {get_param: volume_size}
    image: {get_param: image}
